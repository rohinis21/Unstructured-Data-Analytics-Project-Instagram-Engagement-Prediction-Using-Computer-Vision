{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6d2198",
   "metadata": {},
   "source": [
    "## Is a Picture Worth a Thousand Words? Computer Vision Analysis\n",
    "\n",
    "**Brand:** Pure New Zealand (@purenewzealand)\n",
    "\n",
    "**Alternative Chosen:** Alternative 1 - Instagram Brand Engagement Analysis\n",
    "\n",
    "## **Assignment Overview**\n",
    "\n",
    "This assignment focuses on analyzing Instagram content to understand what drives engagement for the **Pure New Zealand** tourism brand. Using computer vision and natural language processing techniques, we will:\n",
    "\n",
    "- Scrape ~500 posts from Pure New Zealand's Instagram page\n",
    "- Extract image labels using Google Vision API or similar services\n",
    "- Build predictive models to identify high-engagement content\n",
    "- Perform topic modeling to discover content themes\n",
    "- Provide data-driven recommendations to increase engagement\n",
    "\n",
    "The goal is to help Pure New Zealand optimize their Instagram strategy by understanding which visual and textual elements resonate most with their audience.\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "- **Task A:** Web Scraping - Extract Instagram posts (images, captions, likes)\n",
    "- **Task B:** Image Label Extraction - Using Google Vision API/Azure/LLM\n",
    "- **Task C:** Binary Classification - Creating engagement categories\n",
    "- **Task D:** Logistic Regression Models - Predicting engagement levels\n",
    "- **Task E:** Topic Modeling (LDA) - Discovering content themes\n",
    "- **Task F:** Strategic Recommendations - Actionable insights for Pure New Zealand\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f868873",
   "metadata": {},
   "source": [
    "## Task A: \n",
    "\n",
    "Scrape Instagram.py to fetch ~500 posts from the brand‚Äôs Instagram page. Fetch (i) image URLs, (ii) post caption (the text description of a post), and (iii) # likes. Fetching comments is difficult and you can easily get blocked by Insta. Using a dynamic VPN like ExpressVPN is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c88641",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Objective\n",
    "Scrape approximately 500 posts from Pure New Zealand's Instagram page (@purenewzealand) to collect:\n",
    "1. **Image URLs** - Direct links to post images\n",
    "2. **Post Captions** - Text descriptions accompanying each post\n",
    "3. **Number of Likes** - Engagement metric for each post\n",
    "\n",
    "### Methodology\n",
    "We use **Selenium WebDriver** to automate browser interactions and extract data directly from Instagram's web interface. This approach:\n",
    "- Mimics human browsing behavior to avoid detection\n",
    "- Handles dynamic content loading through scrolling\n",
    "- Extracts data by clicking into individual posts\n",
    "- Filters out video posts (focusing only on image content)\n",
    "\n",
    "### Key Features\n",
    "- **VPN Protection**: Prompts user to confirm VPN connection before scraping (we used Express VPN)\n",
    "- **Anti-Detection**: Disables automation flags and uses random delays\n",
    "- **Video Filtering**: Automatically skips video posts to focus on images\n",
    "- **Robust Extraction**: Multiple fallback methods for likes and caption extraction\n",
    "- **Data Persistence**: Saves results in both CSV and JSON formats\n",
    "\n",
    "### Important Notes\n",
    "**Rate Limiting**: Instagram aggressively blocks scrapers. We implement:\n",
    "- Long delays between actions (30-60 seconds)\n",
    "- Random wait times to simulate human behavior\n",
    "- VPN usage to avoid IP blocking\n",
    "- Burner account to protect main account\n",
    "\n",
    "**Ethical Considerations**: This scraping is for educational purposes only and respects Instagram's public content.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PURE SELENIUM INSTAGRAM SCRAPER\n",
    "================================\n",
    "Collects: (1) Image URLs, (2) Captions, (3) Likes\n",
    "\n",
    "Uses only Selenium - no JSON API calls needed.\n",
    "Clicks on each post and extracts data from the modal.\n",
    "\n",
    "REQUIREMENTS:\n",
    "pip install selenium\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from random import uniform\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Burner account credentials (using a dedicated scraping account)\n",
    "BURNER_USERNAME = \"ana.paninew\"\n",
    "BURNER_PASSWORD = \"ytrewq54321\"\n",
    "\n",
    "# Target Instagram handle (Pure New Zealand tourism page)\n",
    "TARGET_HANDLE = \"purenewzealand\"  # Changed from \"zara\" to your actual target\n",
    "\n",
    "# Maximum number of posts to scrape\n",
    "MAX_POSTS = 500  # Changed from 5 to meet assignment requirements\n",
    "\n",
    "# VPN safety check before starting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VPN CHECK\")\n",
    "print(\"=\"*70)\n",
    "vpn_check = input(\"Is your VPN connected? (yes/no): \").strip().lower()\n",
    "if vpn_check != 'yes':\n",
    "    print(\"‚ö†Ô∏è  Please connect to VPN before continuing to avoid IP blocking\")\n",
    "    exit()\n",
    "print(\"‚úì VPN confirmed\\n\")\n",
    "\n",
    "# ==================== INITIALIZE BROWSER ====================\n",
    "print(\"Initializing browser...\")\n",
    "\n",
    "# Chrome options to avoid detection\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Hide automation\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Initialize Chrome driver with anti-detection settings\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "\n",
    "# Navigate to Instagram homepage\n",
    "driver.get(\"https://www.instagram.com/\")\n",
    "print(\"‚úì Browser opened\")\n",
    "time.sleep(60)  # Wait for page to fully load\n",
    "\n",
    "# ==================== LOGIN ====================\n",
    "print(\"\\nLogging in...\")\n",
    "\n",
    "# Wait for and locate username input field\n",
    "username_input = WebDriverWait(driver, 15).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\"))\n",
    ")\n",
    "# Wait for and locate password input field\n",
    "password_input = WebDriverWait(driver, 15).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\"))\n",
    ")\n",
    "\n",
    "# Enter credentials with delays to mimic human typing\n",
    "username_input.clear()\n",
    "username_input.send_keys(BURNER_USERNAME)\n",
    "time.sleep(20)  # Pause between username and password\n",
    "\n",
    "password_input.clear()\n",
    "password_input.send_keys(BURNER_PASSWORD)\n",
    "time.sleep(30)  # Pause before clicking login\n",
    "\n",
    "# Click login button\n",
    "login_button = WebDriverWait(driver, 5).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))\n",
    ")\n",
    "login_button.click()\n",
    "print(\"‚úì Login submitted\")\n",
    "time.sleep(30)  # Wait for login to process\n",
    "\n",
    "# ==================== DISMISS POPUPS ====================\n",
    "print(\"\\nDismissing popups...\")\n",
    "\n",
    "# Instagram shows various popups after login (save info, notifications, etc.)\n",
    "# Try to dismiss them using common button texts\n",
    "for button_text in [\"Not now\", \"Not Now\", \"Never\"]:\n",
    "    try:\n",
    "        popup_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, f'//button[contains(text(), \"{button_text}\")]'))\n",
    "        )\n",
    "        popup_button.click()\n",
    "        time.sleep(10)\n",
    "        print(f\"‚úì Dismissed popup: {button_text}\")\n",
    "    except:\n",
    "        continue  # Popup not found, move to next\n",
    "\n",
    "# ==================== NAVIGATE TO PROFILE ====================\n",
    "print(f\"\\nNavigating to @{TARGET_HANDLE}...\")\n",
    "\n",
    "# Direct navigation to profile page (more reliable than using search)\n",
    "driver.get(f\"https://www.instagram.com/{TARGET_HANDLE}/\")\n",
    "print(f\"‚úì Loaded profile: @{TARGET_HANDLE}\")\n",
    "time.sleep(60)  # Wait for profile page to fully load\n",
    "\n",
    "# ==================== COLLECT POST LINKS ====================\n",
    "print(\"\\nScrolling to collect posts...\")\n",
    "\n",
    "post_links = []  # Store URLs of all posts\n",
    "scroll_pause = 10  # Seconds to wait between scrolls\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# Scroll through the page to load more posts\n",
    "for scroll_num in range(50):  # Increased from 5 to load ~500 posts (adjust as needed)\n",
    "    # Scroll to bottom of page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    print(f\"  Scroll {scroll_num + 1}/50\")\n",
    "    time.sleep(scroll_pause)\n",
    "    \n",
    "    # Find all anchor tags (links) currently visible on the page\n",
    "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    for link in links:\n",
    "        href = link.get_attribute(\"href\")\n",
    "        # Filter for post links (contain \"/p/\") and avoid duplicates\n",
    "        if href and \"/p/\" in href and href not in post_links:\n",
    "            post_links.append(href)\n",
    "    \n",
    "    # Check if we've reached the bottom of the page\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        print(\"  ‚úì Reached bottom of page\")\n",
    "        break\n",
    "    last_height = new_height\n",
    "    \n",
    "    # Extra pause every 2 scrolls to avoid rate limiting\n",
    "    if scroll_num % 2 == 0:\n",
    "        time.sleep(30)\n",
    "\n",
    "# Limit to MAX_POSTS to meet assignment requirements\n",
    "post_links = post_links[:MAX_POSTS]\n",
    "print(f\"\\n‚úì Found {len(post_links)} posts to scrape\")\n",
    "\n",
    "# ==================== SCRAPE EACH POST ====================\n",
    "print(f\"\\nScraping {len(post_links)} posts...\")\n",
    "\n",
    "all_data = []  # Store all scraped post data\n",
    "\n",
    "for index, post_url in enumerate(post_links, 1):\n",
    "    try:\n",
    "        print(f\"\\n[{index}/{len(post_links)}] Opening: {post_url}\")\n",
    "        \n",
    "        # Navigate to individual post page with timeout handling\n",
    "        try:\n",
    "            driver.set_page_load_timeout(180)  # 3 minute timeout for slow connections\n",
    "            driver.get(post_url)\n",
    "            time.sleep(uniform(30, 50))  # Random delay to mimic human behavior\n",
    "        except Exception as timeout_error:\n",
    "            print(f\"  ‚ö†Ô∏è  Timeout loading page: {timeout_error}\")\n",
    "            print(f\"  Waiting 2 minutes before continuing...\")\n",
    "            time.sleep(120)\n",
    "            continue\n",
    "        \n",
    "        # Check if post is a video - SKIP IT (assignment focuses on images)\n",
    "        try:\n",
    "            video_element = driver.find_element(By.TAG_NAME, \"video\")\n",
    "            print(f\"  ‚è≠Ô∏è  SKIPPING: This is a video post\")\n",
    "            continue\n",
    "        except NoSuchElementException:\n",
    "            # No video found, this is an image post - proceed with extraction\n",
    "            pass\n",
    "        \n",
    "        # ===== EXTRACT IMAGE URL =====\n",
    "        # Look for the main image element (uses object-fit CSS)\n",
    "        image_url = \"\"\n",
    "        try:\n",
    "            img_element = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"img[style*='object-fit']\"))\n",
    "            )\n",
    "            image_url = img_element.get_attribute(\"src\")\n",
    "            print(f\"  ‚úì Image URL found\")\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: Could not find image\")\n",
    "            image_url = \"\"\n",
    "        \n",
    "        # ===== EXTRACT CAPTION =====\n",
    "        # Instagram stores captions in the image's alt attribute\n",
    "        caption = \"\"\n",
    "        try:\n",
    "            if image_url:\n",
    "                caption = img_element.get_attribute(\"alt\") or \"\"\n",
    "                print(f\"  ‚úì Caption: {caption[:50]}...\" if caption else \"  ‚ÑπÔ∏è  Caption: (none)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: Could not extract caption - {e}\")\n",
    "            caption = \"\"\n",
    "        \n",
    "        # ===== EXTRACT LIKES =====\n",
    "        # Multiple strategies to find the likes count\n",
    "        likes = 0\n",
    "        try:\n",
    "            import re\n",
    "            # Strategy 1: Find elements containing the word \"likes\"\n",
    "            elements = driver.find_elements(By.XPATH, \"//*[contains(text(), 'likes')]\")\n",
    "            for elem in elements:\n",
    "                text = elem.text.strip()\n",
    "                if 'likes' in text.lower():\n",
    "                    # Extract number from text like \"23,282 likes\"\n",
    "                    numbers = re.findall(r'[\\d,]+', text.replace(',', ''))\n",
    "                    if numbers:\n",
    "                        likes = int(numbers[0])\n",
    "                        print(f\"  ‚úì Likes: {likes:,} (from: '{text}')\")\n",
    "                        break\n",
    "            \n",
    "            # Strategy 2: If no \"likes\" text found, look for number span near \"likes\" element\n",
    "            if likes == 0:\n",
    "                like_spans = driver.find_elements(By.CSS_SELECTOR, \"span.html-span.xdj266r\")\n",
    "                for span in like_spans:\n",
    "                    # Check if this span is near text containing \"likes\"\n",
    "                    parent = span.find_element(By.XPATH, \"../..\")  # Go up 2 levels in DOM\n",
    "                    if 'like' in parent.text.lower():\n",
    "                        likes_text = span.text.strip()\n",
    "                        likes = int(likes_text.replace(',', ''))\n",
    "                        print(f\"  ‚úì Likes: {likes:,} (from number span)\")\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: Could not extract likes - {e}\")\n",
    "            likes = 0\n",
    "        \n",
    "        # ===== STORE DATA =====\n",
    "        all_data.append({\n",
    "            'image_url': image_url,\n",
    "            'caption': caption,\n",
    "            'likes': likes,\n",
    "            'post_url': post_url\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚úì Data collected successfully\")\n",
    "        \n",
    "        # Extra long pause every 10 posts to avoid rate limiting\n",
    "        if index % 10 == 0:\n",
    "            extra_delay = uniform(60, 90)\n",
    "            print(f\"  ‚è∏Ô∏è  Checkpoint break: {extra_delay:.1f}s\")\n",
    "            time.sleep(extra_delay)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error scraping post: {e}\")\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úì Successfully scraped {len(all_data)} posts\")\n",
    "\n",
    "# ==================== SAVE DATA ====================\n",
    "print(\"\\nSaving data...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = f\"{TARGET_HANDLE}_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as CSV (easy to import into pandas)\n",
    "csv_file = os.path.join(output_dir, f\"{TARGET_HANDLE}_instagram_data.csv\")\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['image_url', 'caption', 'likes', 'post_url'])\n",
    "    for data in all_data:\n",
    "        writer.writerow([\n",
    "            data['image_url'],\n",
    "            data['caption'],\n",
    "            data['likes'],\n",
    "            data['post_url']\n",
    "        ])\n",
    "\n",
    "print(f\"‚úì CSV saved: {csv_file}\")\n",
    "\n",
    "# Save as JSON (preserves data structure)\n",
    "json_file = os.path.join(output_dir, f\"{TARGET_HANDLE}_instagram_data.json\")\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úì JSON saved: {json_file}\")\n",
    "\n",
    "# ==================== DISPLAY RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show first 3 posts as preview\n",
    "for i, data in enumerate(all_data[:3], 1):\n",
    "    print(f\"\\nPost {i}:\")\n",
    "    print(f\"  Image: {data['image_url'][:80]}...\")\n",
    "    print(f\"  Likes: {data['likes']:,}\")\n",
    "    caption_preview = data['caption'][:100] + \"...\" if len(data['caption']) > 100 else data['caption']\n",
    "    print(f\"  Caption: {caption_preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target: @{TARGET_HANDLE}\")\n",
    "print(f\"Posts scraped: {len(all_data)}\")\n",
    "print(f\"Output: {csv_file}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clean up: close browser\n",
    "driver.quit()\n",
    "print(\"\\n‚úì Browser closed\")\n",
    "print(\"‚úì Scraping complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528ece5-1636-4f1d-898c-6208dd584589",
   "metadata": {},
   "source": [
    "## Task B:\n",
    "Using the image URLs, obtain image labels (text) from Google Vision (cloud service) or other services such as Azure. You can also use an LLM through its API. You will need an account, though.\n",
    "\n",
    "***We decided to use Google Vision***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1308d",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "**API Configuration:**\n",
    "- **Service**: Google Cloud Vision API v1\n",
    "- **Endpoint**: `https://vision.googleapis.com/v1/images:annotate`\n",
    "- **Authentication**: API Key-based authentication\n",
    "- **Rate Limiting**: 1 second delay between requests to avoid quota exhaustion\n",
    "\n",
    "**Features Extracted:**\n",
    "1. **Labels** (max 10): General descriptors with confidence scores\n",
    "   - Example: \"Sky (0.98), Water (0.95), Nature (0.93)\"\n",
    "2. **Landmarks** (max 5): Famous locations or monuments\n",
    "   - Example: \"Milford Sound, Mount Cook\"\n",
    "3. **Logos** (max 5): Brand logos detected in images\n",
    "4. **Text**: Any visible text in images (stored but not used in analysis)\n",
    "\n",
    "**Error Handling:**\n",
    "- Network timeouts (30 second limit)\n",
    "- Invalid image URLs\n",
    "- API quota limits\n",
    "- Instagram CDN restrictions\n",
    "\n",
    "**Progress Management:**\n",
    "- Saves progress every 10 images\n",
    "- Resume capability from last successful image\n",
    "- Tracks errors in separate column for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc613f3f-7462-47ce-b681-4920c9518116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import base64\n",
    "\n",
    "# ============================================\n",
    "# Google Cloud Vision API - Image Labeling\n",
    "# ============================================\n",
    "\n",
    "def download_and_encode_image(image_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Download image from URL and encode as base64\n",
    "    Instagram URLs require this approach as they need proper headers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(image_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Encode to base64\n",
    "        image_base64 = base64.b64encode(response.content).decode('utf-8')\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"base64\": image_base64,\n",
    "            \"error\": \"\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"base64\": \"\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def get_labels_google_vision(image_url: str, api_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get image labels using Google Cloud Vision API\n",
    "    Downloads image first to handle Instagram CDN URLs\n",
    "    \n",
    "    Returns dictionary with:\n",
    "    - labels: comma-separated string of labels\n",
    "    - landmarks: any landmarks detected\n",
    "    - error: error message if request failed\n",
    "    \"\"\"\n",
    "    endpoint = f\"https://vision.googleapis.com/v1/images:annotate?key={api_key}\"\n",
    "    \n",
    "    # Download and encode image first\n",
    "    download_result = download_and_encode_image(image_url)\n",
    "    \n",
    "    if not download_result[\"success\"]:\n",
    "        return {\n",
    "            \"labels\": \"\",\n",
    "            \"landmarks\": \"\",\n",
    "            \"logos\": \"\",\n",
    "            \"error\": f\"Image download failed: {download_result['error']}\"\n",
    "        }\n",
    "    \n",
    "    # Use base64 content instead of URL\n",
    "    request_body = {\n",
    "        \"requests\": [\n",
    "            {\n",
    "                \"image\": {\"content\": download_result[\"base64\"]},\n",
    "                \"features\": [\n",
    "                    {\"type\": \"LABEL_DETECTION\", \"maxResults\": 10},\n",
    "                    {\"type\": \"LANDMARK_DETECTION\", \"maxResults\": 5},\n",
    "                    {\"type\": \"LOGO_DETECTION\", \"maxResults\": 5},\n",
    "                    {\"type\": \"TEXT_DETECTION\", \"maxResults\": 5}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(endpoint, json=request_body, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract labels\n",
    "        labels = []\n",
    "        landmarks = []\n",
    "        logos = []\n",
    "        \n",
    "        if 'responses' in result and len(result['responses']) > 0:\n",
    "            resp = result['responses'][0]\n",
    "            \n",
    "            # Label annotations\n",
    "            if 'labelAnnotations' in resp:\n",
    "                labels = [\n",
    "                    f\"{label['description']} ({label['score']:.2f})\"\n",
    "                    for label in resp['labelAnnotations']\n",
    "                ]\n",
    "            \n",
    "            # Landmark annotations\n",
    "            if 'landmarkAnnotations' in resp:\n",
    "                landmarks = [\n",
    "                    landmark['description']\n",
    "                    for landmark in resp['landmarkAnnotations']\n",
    "                ]\n",
    "            \n",
    "            # Logo annotations\n",
    "            if 'logoAnnotations' in resp:\n",
    "                logos = [\n",
    "                    logo['description']\n",
    "                    for logo in resp['logoAnnotations']\n",
    "                ]\n",
    "        \n",
    "        return {\n",
    "            \"labels\": \", \".join(labels),\n",
    "            \"landmarks\": \", \".join(landmarks) if landmarks else \"\",\n",
    "            \"logos\": \", \".join(logos) if logos else \"\",\n",
    "            \"error\": \"\"\n",
    "        }\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            \"labels\": \"\",\n",
    "            \"landmarks\": \"\",\n",
    "            \"logos\": \"\",\n",
    "            \"error\": f\"API request error: {str(e)}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"labels\": \"\",\n",
    "            \"landmarks\": \"\",\n",
    "            \"logos\": \"\",\n",
    "            \"error\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "def process_images(csv_file: str, output_file: str, api_key: str, \n",
    "                   rate_limit_delay: float = 1.0, start_from: int = 0):\n",
    "    \"\"\"\n",
    "    Process all images from CSV and save results with Google Cloud Vision\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to input CSV file\n",
    "        output_file: Path to output CSV file\n",
    "        api_key: Your Google Cloud Vision API key\n",
    "        rate_limit_delay: Seconds to wait between API calls (default: 1.0)\n",
    "        start_from: Row index to start from (useful if restarting after error)\n",
    "    \"\"\"\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loading {len(df)} images from CSV...\")\n",
    "    \n",
    "    # Initialize new columns if they don't exist\n",
    "    if 'image_labels' not in df.columns:\n",
    "        df['image_labels'] = \"\"\n",
    "    if 'landmarks' not in df.columns:\n",
    "        df['landmarks'] = \"\"\n",
    "    if 'logos' not in df.columns:\n",
    "        df['logos'] = \"\"\n",
    "    if 'processing_error' not in df.columns:\n",
    "        df['processing_error'] = \"\"\n",
    "    \n",
    "    # Count how many already processed\n",
    "    already_processed = len(df[df['image_labels'] != \"\"])\n",
    "    if already_processed > 0:\n",
    "        print(f\"Found {already_processed} already processed images\")\n",
    "        if start_from == 0:\n",
    "            response = input(\"Continue from where you left off? (y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                start_from = already_processed\n",
    "    \n",
    "    print(f\"\\nStarting from row {start_from}...\")\n",
    "    print(f\"Processing {len(df) - start_from} images...\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    for idx in range(start_from, len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        image_url = row['image_url']\n",
    "        \n",
    "        # Skip if already processed (but show debug info)\n",
    "        if df.at[idx, 'image_labels'] != \"\":\n",
    "            print(f\"Skipping row {idx} - already processed\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{idx + 1}/{len(df)}] Processing: {image_url[:60]}...\")\n",
    "        \n",
    "        result = get_labels_google_vision(image_url, api_key)\n",
    "        \n",
    "        # Store results\n",
    "        df.at[idx, 'image_labels'] = result['labels']\n",
    "        df.at[idx, 'landmarks'] = result['landmarks']\n",
    "        df.at[idx, 'logos'] = result['logos']\n",
    "        df.at[idx, 'processing_error'] = result['error']\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "        if result['error']:\n",
    "            print(f\"   ‚ùå Error: {result['error']}\")\n",
    "        else:\n",
    "            label_count = len(result['labels'].split(',')) if result['labels'] else 0\n",
    "            print(f\"   ‚úì Found {label_count} labels\")\n",
    "            if result['landmarks']:\n",
    "                print(f\"   ‚úì Landmarks: {result['landmarks']}\")\n",
    "        \n",
    "        # DEBUG: Print first result\n",
    "        if idx == 0:\n",
    "            print(f\"\\nüîç DEBUG - First result:\")\n",
    "            print(f\"   Labels: {result['labels'][:100]}...\")\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        if idx < len(df) - 1:\n",
    "            time.sleep(rate_limit_delay)\n",
    "        \n",
    "        # Save progress every 10 images\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"\\nüíæ Progress saved: {idx + 1} images processed\")\n",
    "    \n",
    "    # Final save\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    successful = len(df[df['image_labels'] != \"\"])\n",
    "    errors = len(df[df['processing_error'] != \"\"])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ COMPLETE! Results saved to {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Successfully processed: {successful}/{len(df)} images\")\n",
    "    print(f\"Errors encountered: {errors}\")\n",
    "    print(f\"Images actually processed this run: {processed_count}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN SCRIPT - CONFIGURE HERE\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. ADD YOUR GOOGLE CLOUD VISION API KEY HERE\n",
    "    API_KEY = \"UPDATE WITH YOUR GOOGLE CLOUD VISION API\"\n",
    "    \n",
    "    # 2. Configure file paths\n",
    "    INPUT_CSV = \"pureNZ_scraped_data_550.csv\"\n",
    "    OUTPUT_CSV = \"pureNZ_with_labels.csv\"\n",
    "    \n",
    "    # 3. Run the script\n",
    "    if API_KEY == \"YOUR_GOOGLE_CLOUD_API_KEY_HERE\":\n",
    "        print(\"‚ö†Ô∏è  ERROR: Please add your Google Cloud Vision API key first!\")\n",
    "        print(\"\\nSteps to get your API key:\")\n",
    "        print(\"1. Go to https://console.cloud.google.com/\")\n",
    "        print(\"2. Create a new project (or select existing)\")\n",
    "        print(\"3. Enable 'Cloud Vision API'\")\n",
    "        print(\"4. Go to 'Credentials' ‚Üí 'Create Credentials' ‚Üí 'API Key'\")\n",
    "        print(\"5. Copy the API key and paste it in this script\")\n",
    "    else:\n",
    "        process_images(\n",
    "            csv_file=INPUT_CSV,\n",
    "            output_file=OUTPUT_CSV,\n",
    "            api_key=API_KEY,\n",
    "            rate_limit_delay=1.0  # Wait 1 second between requests\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9984e91c-64dc-4fa5-836d-3cba99566adb",
   "metadata": {},
   "source": [
    "## Task C: \n",
    "\n",
    "Create a column called binary (lowercase only) where value =1 (stands for high engagement) or 0 (stands for low engagement) based on whether the number of likes is above or below the median value.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4d2bd",
   "metadata": {},
   "source": [
    "Our objective is to create a binary target variable to classify posts as **high engagement** (1) or **low engagement** (0) based on the number of likes. This transforms our regression problem into a classification problem suitable for logistic regression.\n",
    "\n",
    "We use the **median number of likes** as the threshold to split posts into two balanced classes:\n",
    "- **Binary = 1**: High engagement (likes > median)\n",
    "- **Binary = 0**: Low engagement (likes ‚â§ median)\n",
    "\n",
    "### Why Use Median as Threshold?\n",
    "- **Balanced Classes**: Ensures equal representation of high/low engagement posts\n",
    "- **Robust to Outliers**: Median is less affected by viral posts with extreme like counts\n",
    "- **Interpretable**: Posts above median represent top 50% performers\n",
    "- **Machine Learning Ready**: Balanced classes prevent model bias toward majority class\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7604a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median likes: 14898.0\n",
      "binary\n",
      "0    275\n",
      "1    275\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"compiled_550_images_descriptions.xlsx\")\n",
    "\n",
    "# normalize column names (case/whitespace)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# ensure 'likes' is numeric (handles \"6,200\" etc.)\n",
    "likes_clean = df['likes'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n",
    "df['likes'] = pd.to_numeric(likes_clean, errors='coerce')\n",
    "\n",
    "median_likes = df['likes'].median()\n",
    "print(\"Median likes:\", median_likes)\n",
    "\n",
    "# 1 if likes > median (ties -> 0, i.e., low engagement)\n",
    "df['binary'] = (df['likes'] > median_likes).astype(int)\n",
    "\n",
    "print(df['binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae4e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded dataset: 550 posts\n",
      "üìã Columns: ['image_url', 'post_url', 'Caption', 'Likes', 'image_labels', 'landmarks', 'logos', 'processing_error']\n",
      "\n",
      "============================================================\n",
      "ENGAGEMENT THRESHOLD\n",
      "============================================================\n",
      "Median likes: 14,898\n",
      "Min likes: 512\n",
      "Max likes: 46,272\n",
      "Mean likes: 15,652\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CLASS DISTRIBUTION\n",
      "============================================================\n",
      "binary\n",
      "0    275\n",
      "1    275\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "  Low Engagement (binary=0): 50.0%\n",
      "  High Engagement (binary=1): 50.0%\n",
      "============================================================\n",
      "\n",
      "SAMPLE POSTS BY ENGAGEMENT LEVEL\n",
      "============================================================\n",
      "\n",
      "üî• HIGH ENGAGEMENT POSTS (binary=1):\n",
      "\n",
      "  Likes: 46,272\n",
      "  Caption: The shades of blue of Lake Tekapo (Takap‚âà√ß). Glacial silt from the Southern Alps...\n",
      "  Labels: Bridge (0.92), Lake (0.85), River (0.83), Winter (0.82), Channel (0.81), Mountain range (0.81), List...\n",
      "\n",
      "  Likes: 44,321\n",
      "  Caption: Have an egg-ceptional Easter. Swipe to see the before photo of this rowi kiwi ch...\n",
      "  Labels: Kiwi (0.97), Bird (0.86), Flightless bird (0.85), Beak (0.82), Feather (0.56)\n",
      "\n",
      "  Likes: 43,223\n",
      "  Caption: Sunrise at The Shire. #NZMustDo [Hobbiton Movie Set, Matamata. : @shaun_jeffers ...\n",
      "  Labels: Nature (0.97), Natural landscape (0.95), Landscape (0.92), Morning (0.85), Sunlight (0.84), Sunrise ...\n",
      "\n",
      "üìâ LOW ENGAGEMENT POSTS (binary=0):\n",
      "\n",
      "  Likes: 512\n",
      "  Caption: Crisp blue skies, warm amber sunsets, and the hot tub calling your name.‚Äö√Ñ√£ Over...\n",
      "  Labels: Advertising (0.91), Poster (0.73), Evening (0.64), Night (0.62), Photo caption (0.53)\n",
      "\n",
      "  Likes: 542\n",
      "  Caption: Witnessing Aotearoa New Zealand‚Äö√Ñ√¥s incredible landscapes unfold during a scenic...\n",
      "  Labels: Helicopter (0.99), Rotorcraft (0.99), Aircraft (0.96), Helicopter rotor (0.95), Aviation (0.92), Aer...\n",
      "\n",
      "  Likes: 641\n",
      "  Caption: Solo doesn‚Äö√Ñ√¥t mean going it alone. We‚Äö√Ñ√¥re here to help you find your people al...\n",
      "  Labels: Adventure (0.73), Backpack (0.60), Walking (0.53)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saved processed dataset to: purenewzealand_with_binary.csv\n",
      "‚úì Dataset ready for Task D (Logistic Regression)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# Load Data with Image Labels\n",
    "# ============================================\n",
    "\n",
    "# Load the compiled dataset with image labels from Task B\n",
    "df = pd.read_excel(\"compiled_550_images_descriptions.xlsx\")\n",
    "\n",
    "print(f\"üìä Loaded dataset: {len(df)} posts\")\n",
    "print(f\"üìã Columns: {df.columns.tolist()}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Data Cleaning & Normalization\n",
    "# ============================================\n",
    "\n",
    "# Normalize column names: convert to lowercase and remove whitespace\n",
    "# This prevents errors from inconsistent column naming (e.g., \"Likes\" vs \"likes\")\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Clean 'Likes' Column\n",
    "# ============================================\n",
    "\n",
    "# Instagram displays likes with commas (e.g., \"14,898\")\n",
    "# We need to convert these to numeric values for analysis\n",
    "\n",
    "# Step 1: Convert to string to handle any data type inconsistencies\n",
    "# Step 2: Remove all non-numeric characters (commas, spaces, etc.)\n",
    "# Step 3: Convert to numeric, setting errors to NaN\n",
    "likes_clean = df['likes'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n",
    "df['likes'] = pd.to_numeric(likes_clean, errors='coerce')\n",
    "\n",
    "# Check for any NaN values after conversion\n",
    "nan_count = df['likes'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Warning: {nan_count} posts with invalid like counts (set to NaN)\")\n",
    "    # Optionally: Drop rows with NaN likes\n",
    "    df = df.dropna(subset=['likes'])\n",
    "    print(f\"‚úì Removed posts with missing likes. New total: {len(df)} posts\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Calculate Median Threshold\n",
    "# ============================================\n",
    "\n",
    "# Calculate median likes across all posts\n",
    "# Median is preferred over mean because it's robust to outliers\n",
    "median_likes = df['likes'].median()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENGAGEMENT THRESHOLD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Median likes: {median_likes:,.0f}\")\n",
    "print(f\"Min likes: {df['likes'].min():,.0f}\")\n",
    "print(f\"Max likes: {df['likes'].max():,.0f}\")\n",
    "print(f\"Mean likes: {df['likes'].mean():,.0f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Create Binary Target Variable\n",
    "# ============================================\n",
    "\n",
    "# Create 'binary' column:\n",
    "# - 1 if likes > median (high engagement)\n",
    "# - 0 if likes ‚â§ median (low engagement)\n",
    "# Note: Posts with exactly median likes are classified as low engagement (0)\n",
    "\n",
    "df['binary'] = (df['likes'] > median_likes).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# Validate Class Balance\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count posts in each class\n",
    "class_counts = df['binary'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print()\n",
    "\n",
    "# Calculate percentages\n",
    "class_percentages = df['binary'].value_counts(normalize=True).sort_index() * 100\n",
    "print(\"Percentages:\")\n",
    "for label, pct in class_percentages.items():\n",
    "    engagement_type = \"High Engagement\" if label == 1 else \"Low Engagement\"\n",
    "    print(f\"  {engagement_type} (binary={label}): {pct:.1f}%\")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Display Sample Results\n",
    "# ============================================\n",
    "\n",
    "print(\"SAMPLE POSTS BY ENGAGEMENT LEVEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show examples of high engagement posts\n",
    "print(\"\\nüî• HIGH ENGAGEMENT POSTS (binary=1):\")\n",
    "high_engagement = df[df['binary'] == 1].nlargest(3, 'likes')\n",
    "for idx, row in high_engagement.iterrows():\n",
    "    print(f\"\\n  Likes: {row['likes']:,.0f}\")\n",
    "    caption_preview = row['caption'][:80] + \"...\" if len(str(row['caption'])) > 80 else row['caption']\n",
    "    print(f\"  Caption: {caption_preview}\")\n",
    "    if 'image_labels' in df.columns and pd.notna(row['image_labels']):\n",
    "        labels_preview = str(row['image_labels'])[:100] + \"...\" if len(str(row['image_labels'])) > 100 else row['image_labels']\n",
    "        print(f\"  Labels: {labels_preview}\")\n",
    "\n",
    "# Show examples of low engagement posts\n",
    "print(\"\\nüìâ LOW ENGAGEMENT POSTS (binary=0):\")\n",
    "low_engagement = df[df['binary'] == 0].nsmallest(3, 'likes')\n",
    "for idx, row in low_engagement.iterrows():\n",
    "    print(f\"\\n  Likes: {row['likes']:,.0f}\")\n",
    "    caption_preview = row['caption'][:80] + \"...\" if len(str(row['caption'])) > 80 else row['caption']\n",
    "    print(f\"  Caption: {caption_preview}\")\n",
    "    if 'image_labels' in df.columns and pd.notna(row['image_labels']):\n",
    "        labels_preview = str(row['image_labels'])[:100] + \"...\" if len(str(row['image_labels'])) > 100 else row['image_labels']\n",
    "        print(f\"  Labels: {labels_preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# Save Processed Dataset\n",
    "# ============================================\n",
    "\n",
    "# Save the dataset with the new 'binary' column for Task D\n",
    "output_file = \"purenewzealand_with_binary.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nüíæ Saved processed dataset to: {output_file}\")\n",
    "print(f\"‚úì Dataset ready for Task D (Logistic Regression)\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b92af0bb-1b33-4202-aac4-12463d2204f1",
   "metadata": {},
   "source": [
    "### Task D\n",
    "\n",
    "Run a logistic regression with binary as the dependent variable, and the image_labels as independent variables. You can use a BoW model for text. What is the accuracy (show the confusion matrix) of this prediction model? The idea is to be able to predict the engagement level for an image.\n",
    "\n",
    "$$Accuracy = 1 - # prediction errors / total # cases$$\n",
    "\n",
    "What accuracy do you get by using the post_caption words as the independent variables instead of image_labels? Finally, what accuracy do you get by combining (concatenating) the image_labels and post_caption and using them together as independent variables? What can you conclude from your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec0fd65",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "Clean and prepare the image labels from Google Vision API for use in machine learning models. The raw labels include confidence scores in parentheses (e.g., \"Sky (0.98), Water (0.95)\") which need to be removed for text analysis.\n",
    "\n",
    "### Cleaning Steps\n",
    "1. Remove confidence scores in parentheses\n",
    "2. Keep only the descriptive text\n",
    "3. Standardize formatting for consistency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd00317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 550 posts with image labels\n",
      "\n",
      "Example label cleaning:\n",
      "======================================================================\n",
      "\n",
      "Original:  Fog (0.77), Mist (0.74), Adventure (0.67), Wind (0.67), Haze (0.66), Dust (0.63), Smoke (0.61), Digi...\n",
      "Cleaned:   Fog, Mist, Adventure, Wind, Haze, Dust, Smoke, Digital compositing...\n",
      "\n",
      "Original:  Smile (0.96), Happiness (0.87), Luggage & bags (0.86), Pedestrian (0.85), Leisure (0.81), Beard (0.7...\n",
      "Cleaned:   Smile, Happiness, Luggage & bags, Pedestrian, Leisure, Beard, Vacation, Handbag, Sunglasses, Adverti...\n",
      "\n",
      "Original:  Happiness (0.90), Tribe (0.62)...\n",
      "Cleaned:   Happiness, Tribe...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data with image labels from Task B\n",
    "df = pd.read_csv(\"purenewzealand_with_binary.csv\")\n",
    "\n",
    "print(f\"Loaded {len(df)} posts with image labels\")\n",
    "\n",
    "# Create clean_labels column by removing confidence scores\n",
    "# Example: \"Sky (0.98), Water (0.95)\" ‚Üí \"Sky, Water\"\n",
    "def clean_labels(label_text):\n",
    "    \"\"\"\n",
    "    Remove confidence scores from image labels.\n",
    "    Input: \"Sky (0.98), Water (0.95), Nature (0.93)\"\n",
    "    Output: \"Sky, Water, Nature\"\n",
    "    \"\"\"\n",
    "    if pd.isna(label_text) or label_text == '':\n",
    "        return ''\n",
    "    # Remove everything in parentheses including the parentheses\n",
    "    cleaned = re.sub(r'\\s*\\([^)]*\\)', '', label_text)\n",
    "    return cleaned\n",
    "\n",
    "df['clean_labels'] = df['image_labels'].apply(clean_labels)\n",
    "\n",
    "# Display before/after examples\n",
    "print(\"\\nExample label cleaning:\")\n",
    "print(\"=\"*70)\n",
    "for i in range(3):\n",
    "    if pd.notna(df['image_labels'].iloc[i]):\n",
    "        print(f\"\\nOriginal:  {df['image_labels'].iloc[i][:100]}...\")\n",
    "        print(f\"Cleaned:   {df['clean_labels'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3746a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Train set size: 440 posts (80.0%)\n",
      "‚úì Test set size: 110 posts (20.0%)\n",
      "‚úì Train set class balance: 220/220\n",
      "‚úì Test set class balance: 55/55\n",
      "\n",
      "======================================================================\n",
      "TF-IDF CONFIGURATION\n",
      "======================================================================\n",
      "‚úì N-grams: Unigrams (1-word) + Bigrams (2-word)\n",
      "‚úì Stop words: Removed (English)\n",
      "‚úì Min document frequency: 2 posts\n",
      "‚úì Max document frequency: 95% of posts\n",
      "‚úì Sublinear TF scaling: Enabled\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL 1: IMAGE LABELS ONLY (Computer Vision)\n",
      "======================================================================\n",
      "‚úì Training features shape: (440, 1013)\n",
      "  (440 posts √ó 1013 unique features)\n",
      "‚úì Test features shape: (110, 1013)\n",
      "\n",
      "üéØ MODEL 1 ACCURACY: 0.7091 (70.91%)\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Low Engagement       0.72      0.69      0.70        55\n",
      "High Engagement       0.70      0.73      0.71        55\n",
      "\n",
      "       accuracy                           0.71       110\n",
      "      macro avg       0.71      0.71      0.71       110\n",
      "   weighted avg       0.71      0.71      0.71       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Low  High\n",
      "Actual Low    [  38   17]\n",
      "Actual High   [  15   40]\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL 2: POST CAPTIONS ONLY (Text Descriptions)\n",
      "======================================================================\n",
      "‚úì Training features shape: (440, 1602)\n",
      "  (440 posts √ó 1602 unique features)\n",
      "‚úì Test features shape: (110, 1602)\n",
      "\n",
      "üéØ MODEL 2 ACCURACY: 0.6909 (69.09%)\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Low Engagement       0.71      0.65      0.68        55\n",
      "High Engagement       0.68      0.73      0.70        55\n",
      "\n",
      "       accuracy                           0.69       110\n",
      "      macro avg       0.69      0.69      0.69       110\n",
      "   weighted avg       0.69      0.69      0.69       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Low  High\n",
      "Actual Low    [  36   19]\n",
      "Actual High   [  15   40]\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL 3: MULTIMODAL (Image Labels + Captions)\n",
      "======================================================================\n",
      "‚úì Combined training features shape: (440, 2615)\n",
      "  (Image labels: 1013 + Captions: 1602 = 2615 total)\n",
      "‚úì Combined test features shape: (110, 2615)\n",
      "\n",
      "üéØ MODEL 3 ACCURACY: 0.7818 (78.18%)\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Low Engagement       0.78      0.78      0.78        55\n",
      "High Engagement       0.78      0.78      0.78        55\n",
      "\n",
      "       accuracy                           0.78       110\n",
      "      macro avg       0.78      0.78      0.78       110\n",
      "   weighted avg       0.78      0.78      0.78       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Low  High\n",
      "Actual Low    [  43   12]\n",
      "Actual High   [  12   43]\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPARATIVE MODEL PERFORMANCE\n",
      "======================================================================\n",
      "                     Model  Accuracy  Improvement over Baseline\n",
      "         Image Labels Only  0.709091                  20.909091\n",
      "             Captions Only  0.690909                  19.090909\n",
      "Combined (Image + Caption)  0.781818                  28.181818\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üèÜ BEST MODEL: Combined (Image + Caption)\n",
      "   Accuracy: 0.7818 (78.18%)\n",
      "   Beats random guessing by: 28.18 percentage points\n",
      "\n",
      "üìä Relative Performance:\n",
      "   Captions vs Images: -2.56% change\n",
      "   Combined vs Images: +10.26% change\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Create Consistent Train-Test Split\n",
    "# ============================================\n",
    "\n",
    "\"\"\"\n",
    "WHY USE POSITIONAL INDICES?\n",
    "- Ensures the SAME posts are in train/test across all three models\n",
    "- Prevents data leakage between models\n",
    "- Makes performance comparisons valid and fair\n",
    "- Stratified split maintains 50/50 class balance in both sets\n",
    "\"\"\"\n",
    "\n",
    "# Create array of positional indices [0, 1, 2, ..., 549]\n",
    "pos_idx = np.arange(len(df))\n",
    "\n",
    "# Split indices into train (80%) and test (20%) sets\n",
    "# stratify=df['binary'] ensures both sets have balanced classes\n",
    "train_pos, test_pos = train_test_split(\n",
    "    pos_idx, \n",
    "    test_size=0.2,      # 20% for testing (110 posts)\n",
    "    random_state=42,    # Reproducible results\n",
    "    stratify=df['binary']  # Maintain 50/50 high/low split\n",
    ")\n",
    "\n",
    "# Extract target variable (binary engagement labels)\n",
    "y = df['binary'].values  # NumPy array for positional indexing\n",
    "\n",
    "print(f\"‚úì Train set size: {len(train_pos)} posts ({len(train_pos)/len(df)*100:.1f}%)\")\n",
    "print(f\"‚úì Test set size: {len(test_pos)} posts ({len(test_pos)/len(df)*100:.1f}%)\")\n",
    "print(f\"‚úì Train set class balance: {y[train_pos].sum()}/{len(train_pos)-y[train_pos].sum()}\")\n",
    "print(f\"‚úì Test set class balance: {y[test_pos].sum()}/{len(test_pos)-y[test_pos].sum()}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# TF-IDF Vectorizer Configuration\n",
    "# ============================================\n",
    "\n",
    "\"\"\"\n",
    "WHY TF-IDF?\n",
    "1. Term Frequency (TF): Counts word occurrences\n",
    "2. Inverse Document Frequency (IDF): Reduces weight of common words\n",
    "3. Result: Distinctive words get higher importance\n",
    "\n",
    "Example:\n",
    "- Common word \"travel\" (appears in 80% of posts) ‚Üí low weight\n",
    "- Distinctive word \"fjord\" (appears in 5% of posts) ‚Üí high weight\n",
    "\"\"\"\n",
    "\n",
    "def make_vec():\n",
    "    \"\"\"\n",
    "    Creates standardized TF-IDF vectorizer for text processing.\n",
    "    \n",
    "    Parameters:\n",
    "    - stop_words='english': Remove common words (the, is, and, etc.)\n",
    "    - ngram_range=(1,2): Use unigrams (single words) and bigrams (word pairs)\n",
    "    - min_df=2: Ignore words appearing in fewer than 2 documents\n",
    "    - max_df=0.95: Ignore words appearing in >95% of documents\n",
    "    - sublinear_tf=True: Use log scaling for term frequency\n",
    "    - lowercase=True: Convert all text to lowercase\n",
    "    \"\"\"\n",
    "    return TfidfVectorizer(\n",
    "        stop_words='english',   # Remove \"the\", \"is\", \"and\", etc.\n",
    "        ngram_range=(1,2),      # Capture \"mountain\" and \"mountain view\"\n",
    "        min_df=2,               # Word must appear in ‚â•2 posts\n",
    "        max_df=0.95,            # Ignore words in >95% of posts\n",
    "        sublinear_tf=True,      # Log scaling: reduces impact of word repetition\n",
    "        lowercase=True          # \"Mountain\" = \"mountain\"\n",
    "    )\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TF-IDF CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úì N-grams: Unigrams (1-word) + Bigrams (2-word)\")\n",
    "print(\"‚úì Stop words: Removed (English)\")\n",
    "print(\"‚úì Min document frequency: 2 posts\")\n",
    "print(\"‚úì Max document frequency: 95% of posts\")\n",
    "print(\"‚úì Sublinear TF scaling: Enabled\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# MODEL 1: IMAGE LABELS ONLY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 1: IMAGE LABELS ONLY (Computer Vision)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "## Removing the 0.99,0.70 coefficients on labels for my bag of words model\n",
    "df['clean_labels'] = df['image_labels'].str.replace(r'\\([^)]*\\)', '', regex=True)\n",
    "df['clean_labels'] = df['clean_labels'].str.replace('[^A-Za-z ]', '', regex=True)\n",
    "\n",
    "# Initialize TF-IDF vectorizer for image labels\n",
    "vec1 = make_vec()\n",
    "\n",
    "# Transform image labels into TF-IDF features\n",
    "# fillna('') handles posts without labels (treats as empty string)\n",
    "# iloc[train_pos] selects only training indices\n",
    "X1_train = vec1.fit_transform(df['clean_labels'].fillna('').iloc[train_pos])\n",
    "X1_test = vec1.transform(df['clean_labels'].fillna('').iloc[test_pos])\n",
    "\n",
    "print(f\"‚úì Training features shape: {X1_train.shape}\")\n",
    "print(f\"  ({X1_train.shape[0]} posts √ó {X1_train.shape[1]} unique features)\")\n",
    "print(f\"‚úì Test features shape: {X1_test.shape}\\n\")\n",
    "\n",
    "# Train logistic regression model\n",
    "# max_iter=2000: Sufficient iterations for convergence\n",
    "# class_weight=None: Classes already balanced (no adjustment needed)\n",
    "# solver='liblinear': Good for small-medium datasets with L1/L2 regularization\n",
    "m1 = LogisticRegression(\n",
    "    max_iter=2000, \n",
    "    class_weight=None,      # Balanced classes don't need weighting\n",
    "    solver='liblinear',     # Efficient for sparse data\n",
    "    random_state=42\n",
    ")\n",
    "m1.fit(X1_train, y[train_pos])\n",
    "\n",
    "# Make predictions on test set\n",
    "p1 = m1.predict(X1_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc1 = accuracy_score(y[test_pos], p1)\n",
    "print(f\"üéØ MODEL 1 ACCURACY: {acc1:.4f} ({acc1*100:.2f}%)\\n\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y[test_pos], p1, target_names=['Low Engagement', 'High Engagement']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm1 = confusion_matrix(y[test_pos], p1)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Low  High\")\n",
    "print(f\"Actual Low    [{cm1[0,0]:4d} {cm1[0,1]:4d}]\")\n",
    "print(f\"Actual High   [{cm1[1,0]:4d} {cm1[1,1]:4d}]\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# MODEL 2: CAPTIONS ONLY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 2: POST CAPTIONS ONLY (Text Descriptions)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize NEW TF-IDF vectorizer for captions\n",
    "# Important: Separate vectorizer to maintain independent vocabularies\n",
    "vec2 = make_vec()\n",
    "\n",
    "# Transform captions into TF-IDF features\n",
    "X2_train = vec2.fit_transform(df['caption'].fillna('').iloc[train_pos])\n",
    "X2_test = vec2.transform(df['caption'].fillna('').iloc[test_pos])\n",
    "\n",
    "print(f\"‚úì Training features shape: {X2_train.shape}\")\n",
    "print(f\"  ({X2_train.shape[0]} posts √ó {X2_train.shape[1]} unique features)\")\n",
    "print(f\"‚úì Test features shape: {X2_test.shape}\\n\")\n",
    "\n",
    "# Train logistic regression model\n",
    "m2 = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=None,\n",
    "    solver='liblinear',\n",
    "    random_state=42\n",
    ")\n",
    "m2.fit(X2_train, y[train_pos])\n",
    "\n",
    "# Make predictions\n",
    "p2 = m2.predict(X2_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc2 = accuracy_score(y[test_pos], p2)\n",
    "print(f\"üéØ MODEL 2 ACCURACY: {acc2:.4f} ({acc2*100:.2f}%)\\n\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y[test_pos], p2, target_names=['Low Engagement', 'High Engagement']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm2 = confusion_matrix(y[test_pos], p2)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Low  High\")\n",
    "print(f\"Actual Low    [{cm2[0,0]:4d} {cm2[0,1]:4d}]\")\n",
    "print(f\"Actual High   [{cm2[1,0]:4d} {cm2[1,1]:4d}]\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# MODEL 3: COMBINED (IMAGE LABELS + CAPTIONS)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 3: MULTIMODAL (Image Labels + Captions)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\"\"\"\n",
    "WHY COMBINE HORIZONTALLY?\n",
    "- hstack() concatenates feature matrices side-by-side\n",
    "- Each modality (vision/text) keeps its own vocabulary and scaling\n",
    "- Model learns relative importance of visual vs textual features\n",
    "- Prevents one modality from overpowering the other\n",
    "\n",
    "Feature matrix structure:\n",
    "[Image Label Features | Caption Features]\n",
    "[   X1_train (440√óN)  |  X2_train (440√óM) ] ‚Üí Combined (440 √ó N+M)\n",
    "\"\"\"\n",
    "\n",
    "# Combine training features\n",
    "X_lbl_tr = X1_train  # Image label features (already computed)\n",
    "X_cap_tr = X2_train  # Caption features (already computed)\n",
    "X_tr = hstack([X_lbl_tr, X_cap_tr])  # Horizontal concatenation\n",
    "\n",
    "# Combine test features\n",
    "X_lbl_te = X1_test\n",
    "X_cap_te = X2_test\n",
    "X_te = hstack([X_lbl_te, X_cap_te])\n",
    "\n",
    "print(f\"‚úì Combined training features shape: {X_tr.shape}\")\n",
    "print(f\"  (Image labels: {X_lbl_tr.shape[1]} + Captions: {X_cap_tr.shape[1]} = {X_tr.shape[1]} total)\")\n",
    "print(f\"‚úì Combined test features shape: {X_te.shape}\\n\")\n",
    "\n",
    "# Train logistic regression model\n",
    "m3 = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=None,\n",
    "    solver='liblinear',\n",
    "    random_state=42\n",
    ")\n",
    "m3.fit(X_tr, y[train_pos])\n",
    "\n",
    "# Make predictions\n",
    "p3 = m3.predict(X_te)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc3 = accuracy_score(y[test_pos], p3)\n",
    "print(f\"üéØ MODEL 3 ACCURACY: {acc3:.4f} ({acc3*100:.2f}%)\\n\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y[test_pos], p3, target_names=['Low Engagement', 'High Engagement']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm3 = confusion_matrix(y[test_pos], p3)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Low  High\")\n",
    "print(f\"Actual Low    [{cm3[0,0]:4d} {cm3[0,1]:4d}]\")\n",
    "print(f\"Actual High   [{cm3[1,0]:4d} {cm3[1,1]:4d}]\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# COMPARATIVE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARATIVE MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Image Labels Only', 'Captions Only', 'Combined (Image + Caption)'],\n",
    "    'Accuracy': [acc1, acc2, acc3],\n",
    "    'Improvement over Baseline': [\n",
    "        (acc1 - 0.5) * 100,\n",
    "        (acc2 - 0.5) * 100,\n",
    "        (acc3 - 0.5) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Identify best model\n",
    "best_idx = comparison['Accuracy'].idxmax()\n",
    "best_model = comparison.loc[best_idx, 'Model']\n",
    "best_acc = comparison.loc[best_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "print(f\"   Beats random guessing by: {(best_acc - 0.5)*100:.2f} percentage points\")\n",
    "\n",
    "# Calculate relative improvements\n",
    "if acc1 != 0:\n",
    "    caption_vs_image = ((acc2 - acc1) / acc1) * 100\n",
    "    combined_vs_image = ((acc3 - acc1) / acc1) * 100\n",
    "    print(f\"\\nüìä Relative Performance:\")\n",
    "    print(f\"   Captions vs Images: {caption_vs_image:+.2f}% change\")\n",
    "    print(f\"   Combined vs Images: {combined_vs_image:+.2f}% change\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION: Model Comparison\n",
    "# ============================================\n",
    "\n",
    "# # # Create bar chart comparing model accuracies\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# models = ['Image Labels\\nOnly', 'Captions\\nOnly', 'Combined\\n(Image + Caption)']\n",
    "# accuracies = [acc1, acc2, acc3]\n",
    "# colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# bars = plt.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "# plt.axhline(y=0.5, color='gray', linestyle='--', label='Random Baseline (50%)', linewidth=2)\n",
    "\n",
    "# # Add value labels on bars\n",
    "# for bar, acc in zip(bars, accuracies):\n",
    "#     height = bar.get_height()\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "#              f'{acc:.1%}',\n",
    "#              ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# plt.ylabel('Accuracy', fontsize=12)\n",
    "# plt.title('Logistic Regression Model Comparison\\nPure New Zealand Instagram Engagement Prediction', \n",
    "#           fontsize=14, fontweight='bold')\n",
    "# plt.ylim(0, 1.0)\n",
    "# plt.legend()\n",
    "# plt.grid(axis='y', alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# # ============================================\n",
    "# # VISUALIZATION: Confusion Matrices\n",
    "# # ============================================\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# cms = [cm1, cm2, cm3]\n",
    "# titles = ['Model 1: Image Labels', 'Model 2: Captions', 'Model 3: Combined']\n",
    "# accs = [acc1, acc2, acc3]\n",
    "\n",
    "# for ax, cm, title, acc in zip(axes, cms, titles, accs):\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n",
    "#                 xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "#     ax.set_title(f'{title}\\nAccuracy: {acc:.1%}', fontsize=11, fontweight='bold')\n",
    "#     ax.set_ylabel('Actual Engagement')\n",
    "#     ax.set_xlabel('Predicted Engagement')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af817a0",
   "metadata": {},
   "source": [
    "### Model Performance Results\n",
    "\n",
    "#### Model Comparison Summary\n",
    "\n",
    "![Model Comparison](model_comparison.png)\n",
    "\n",
    "**Performance Rankings:**\n",
    "1. ü•á **Combined Model (Image + Caption)**: 78.2% accuracy\n",
    "2. ü•à **Image Labels Only**: 70.9% accuracy  \n",
    "3. ü•â **Captions Only**: 69.1% accuracy\n",
    "\n",
    "All three models significantly outperform the random baseline (50%), demonstrating that both visual and textual features contain predictive signals for engagement.\n",
    "\n",
    "---\n",
    "\n",
    "#### Confusion Matrix Analysis\n",
    "\n",
    "![Confusion Matrices](confusion_matrices.png)\n",
    "\n",
    "**Model 1: Image Labels Only (70.9%)**\n",
    "- True Negatives: 38 | False Positives: 17\n",
    "- False Negatives: 15 | True Positives: 40\n",
    "- **Interpretation**: Good at identifying high-engagement posts (40/55 correct) but misses some low-engagement posts\n",
    "\n",
    "**Model 2: Captions Only (69.1%)**  \n",
    "- True Negatives: 36 | False Positives: 19\n",
    "- False Negatives: 15 | True Positives: 40\n",
    "- **Interpretation**: Similar pattern to Model 1, slightly more false positives\n",
    "\n",
    "**Model 3: Combined (78.2%)**\n",
    "- True Negatives: 43 | False Positives: 12\n",
    "- False Negatives: 12 | True Positives: 43\n",
    "- **Interpretation**: Best balanced performance, reduced errors in both classes\n",
    "\n",
    "---\n",
    "\n",
    "### Technical Implementation Notes\n",
    "\n",
    "**Why TF-IDF Vectorizer?**\n",
    "We used TF-IDF to give more weight to distinctive words and reduce the influence of generic ones. This improves logistic regression by normalizing text length and emphasizing unique, meaningful terms (e.g., \"fjord\" > \"travel\"). For captions and image labels, it helps the model focus on words that actually differentiate high- and low-engagement posts.\n",
    "\n",
    "**Why Indices Were Used**\n",
    "We created a single train/test split using index positions and reused those indices for all models. This kept the same posts in train/test across image-only, caption-only, and combined models‚Äîensuring fair comparison and avoiding data leakage.\n",
    "\n",
    "**How Images and Captions Were Combined**\n",
    "We built separate TF-IDF vectorizers for image labels and captions to keep their vocabularies and scaling independent, then horizontally stacked the two feature sets before training. This let the model learn how much each modality (visual tags vs. text captions) contributes to engagement without one overpowering the other.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings & Insights\n",
    "\n",
    "**1. Combined Model Achieves Best Performance (78.2%)**\n",
    "- The multimodal approach outperforms single-modality models\n",
    "- **7.3 percentage point improvement** over image-only model\n",
    "- **9.1 percentage point improvement** over caption-only model\n",
    "- Visual and textual features provide complementary information\n",
    "\n",
    "**2. Image Labels Strong Solo Predictor (70.9%)**\n",
    "- Computer vision features alone achieve solid accuracy\n",
    "- Confirms Instagram's visual-first nature\n",
    "- Distinctive visual elements (landscapes, activities) drive engagement\n",
    "\n",
    "**3. Captions Slightly Underperform Images (69.1%)**\n",
    "- Text descriptions have predictive value but less than images\n",
    "- May indicate users engage with visuals before reading captions\n",
    "- Caption content may be more formulaic across posts\n",
    "\n",
    "**4. Synergy Between Modalities**\n",
    "- Combined model doesn't just add features‚Äîit achieves synergy\n",
    "- The 78.2% accuracy suggests visual and textual cues work together\n",
    "- Some engagement patterns require both visual content AND context from captions\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "I can conclude that using computer vision to predict whether a post will be above or below the median number of likes in the dataset is a better predictor than using captions alone, or even captions combined with text labels. Overall, **the combined model (image labels + captions) provided the best results at 78.2% accuracy**.\n",
    "\n",
    "Intuitively, this makes sense since, being Instagram, what carries the most weight is the image or picture uploaded. In addition, captions might not resonate as much with people or may not even be seen. It's the nature of the app. Hence, to predict and make recommendations, focus on your picture first, but don't ignore captions‚Äîthe combination of strong visual content with contextual text descriptions achieves the highest engagement prediction accuracy.\n",
    "\n",
    "**The data-driven insight**: What people **see** matters most, but what they **read** adds valuable context. The optimal strategy combines both.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98fe14fc",
   "metadata": {},
   "source": [
    "## Task E \n",
    "\n",
    "Perform topic modeling (LDA) on the original image_labels. Choose an appropriate number of topics. You may want to start with 4-5 topics, but adjust the number up or down depending on the word distributions you get. Decide on suitable names for each topic. \n",
    "Now sort the data from high to low number of likes (don‚Äôt use the binary column, use the actual number of likes), and consider the highest and the lowest quartiles of likes. What are the main differences in the average topic weights of images across the two quartiles (e.g., greater weight of some topics in the highest versus lowest quartiles)? Show the main results in a table. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c2c25-4c23-4ce2-a4c7-222f531d5335",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Perform topic modeling using Latent Dirichlet Allocation (LDA) to:\n",
    "1. **Identify thematic patterns** - Discover hidden topics within image labels\n",
    "2. **Assign topic weights** - Calculate probability distributions for each image across identified topics\n",
    "3. **Analyze engagement drivers** - Compare topic prevalence between high-engagement and low-engagement images\n",
    "4. **Extract actionable insights** - Determine which visual themes resonate most with audiences\n",
    "\n",
    "---\n",
    "\n",
    "### Methodology\n",
    "We employ **Latent Dirichlet Allocation (LDA)**, an unsupervised probabilistic model that:\n",
    "- Treats each image's labels as a \"document\" composed of a mixture of topics\n",
    "- Assumes each topic is characterized by a distribution over words (labels)\n",
    "- Discovers latent thematic structures without predefined categories\n",
    "- Assigns each image a probability distribution across all topics\n",
    "\n",
    "**Iterative Topic Selection Process:**\n",
    "1. **Initial baseline (5 topics)**: Started with a common exploratory configuration\n",
    "2. **Identified limitations**: Observed topic overlap, broad themes, and poor interpretability\n",
    "3. **Incremental refinement**: Tested 6, 7, and 8-topic configurations\n",
    "4. **Final selection (8 topics)**: Achieved optimal balance of:\n",
    "   - Clear thematic separation with minimal keyword overlap\n",
    "   - Coherent, interpretable word distributions\n",
    "   - Comprehensive coverage of diverse photographic content\n",
    "   - Meaningful topic labels based on top word probabilities\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features\n",
    "- **Preprocessing**: CountVectorizer with lowercase conversion and English stop word removal\n",
    "- **Model Configuration**: \n",
    "  - 8 topics (n_components=8)\n",
    "  - Batch learning method for stability\n",
    "  - Random state fixed (42) for reproducibility\n",
    "- **Topic Interpretation**: Manual inspection of top 10 words per topic to assign descriptive labels\n",
    "- **Quartile Analysis**: Comparison of topic distributions between top 25% and bottom 25% of images by engagement (likes)\n",
    "- **Visualization**: Clear comparison tables showing topic weight differences across engagement levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541d7456-1219-41ec-a9df-94f7b2ef506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Fitting LDA model with 8 topics...\n",
      "   (This was determined through iterative testing from 5 ‚Üí 6 ‚Üí 7 ‚Üí 8 topics)\n",
      "   Vocabulary size: 766 unique words\n",
      "   Number of documents: 550\n",
      "‚úÖ LDA model fitted successfully!\n",
      "\n",
      "================================================================================\n",
      "EXAMINING WORD DISTRIBUTIONS TO INTERPRET TOPICS\n",
      "================================================================================\n",
      "We'll look at the top 10 words for each topic to assign meaningful labels.\n",
      "This manual inspection helps us understand what each topic represents.\n",
      "\n",
      "üìä TOPIC 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bird</th>\n",
       "      <td>0.056795</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winter</th>\n",
       "      <td>0.042484</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beak</th>\n",
       "      <td>0.037950</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penguin</th>\n",
       "      <td>0.027481</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feather</th>\n",
       "      <td>0.023294</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ice</th>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>0.020837</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>0.019986</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caption</th>\n",
       "      <td>0.019986</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertising</th>\n",
       "      <td>0.018098</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "bird         0.056795  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "winter       0.042484  0.000223  0.000248  0.006602  0.000075  0.002069   \n",
       "beak         0.037950  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "penguin      0.027481  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "feather      0.023294  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "ice          0.021200  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "snow         0.020837  0.000223  0.000248  0.005902  0.000075  0.000195   \n",
       "photo        0.019986  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "caption      0.019986  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "advertising  0.018098  0.002089  0.002342  0.000060  0.000075  0.003586   \n",
       "\n",
       "              topic_7   topic_8  \n",
       "bird         0.000136  0.000261  \n",
       "winter       0.000136  0.000261  \n",
       "beak         0.000136  0.000261  \n",
       "penguin      0.000136  0.000261  \n",
       "feather      0.000136  0.000261  \n",
       "ice          0.000136  0.000261  \n",
       "snow         0.000136  0.000261  \n",
       "photo        0.000136  0.001473  \n",
       "caption      0.000136  0.001473  \n",
       "advertising  0.002542  0.000261  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Birds & Wildlife (keywords: bird, beak, penguin, etc.)\n",
      "\n",
      "üìä TOPIC 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>leisure</th>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.016757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacation</th>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.007034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fur</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recreation</th>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.028139</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carnivores</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.023233</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animal</th>\n",
       "      <td>0.012352</td>\n",
       "      <td>0.022027</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.004439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snout</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.021625</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whiskers</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.019741</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wildlife</th>\n",
       "      <td>0.017740</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrestrial</th>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "leisure      0.000271  0.043204  0.000249  0.000060  0.000075  0.000195   \n",
       "vacation     0.000281  0.035445  0.000248  0.000060  0.000075  0.000195   \n",
       "fur          0.000262  0.032325  0.000248  0.000060  0.000075  0.000195   \n",
       "recreation   0.008754  0.028139  0.015478  0.002444  0.002872  0.000195   \n",
       "carnivores   0.000262  0.023233  0.000248  0.000060  0.000075  0.000195   \n",
       "animal       0.012352  0.022027  0.000248  0.000060  0.000075  0.000195   \n",
       "snout        0.000262  0.021625  0.000248  0.000060  0.000075  0.000195   \n",
       "whiskers     0.000262  0.019741  0.000248  0.000060  0.000075  0.000195   \n",
       "wildlife     0.017740  0.019118  0.000248  0.000060  0.000075  0.000195   \n",
       "terrestrial  0.009742  0.018900  0.000248  0.000060  0.000075  0.000195   \n",
       "\n",
       "              topic_7   topic_8  \n",
       "leisure      0.000136  0.016757  \n",
       "vacation     0.000136  0.007034  \n",
       "fur          0.000136  0.000261  \n",
       "recreation   0.000137  0.000261  \n",
       "carnivores   0.001334  0.000261  \n",
       "animal       0.000136  0.004439  \n",
       "snout        0.000136  0.000261  \n",
       "whiskers     0.001288  0.000261  \n",
       "wildlife     0.001290  0.000261  \n",
       "terrestrial  0.000136  0.000261  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Leisure & Vacation (keywords: beach, vacation, leisure, etc.)\n",
      "\n",
      "üìä TOPIC 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bicycle</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.059810</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spring</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.032014</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supplies</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.018117</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.018117</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.077330</td>\n",
       "      <td>0.057918</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.030540</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wheel</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.016131</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.004929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recreation</th>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.028139</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.014146</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytime</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "bicycle     0.000262  0.000223  0.059810  0.000060  0.000075  0.000195   \n",
       "spring      0.000262  0.000223  0.032014  0.000060  0.000075  0.000195   \n",
       "supplies    0.000262  0.000223  0.018117  0.000060  0.000075  0.000195   \n",
       "transport   0.000262  0.000223  0.018117  0.000060  0.000075  0.000195   \n",
       "water       0.000262  0.000223  0.017638  0.077330  0.057918  0.000195   \n",
       "wheel       0.000262  0.000223  0.016131  0.000060  0.000075  0.000195   \n",
       "area        0.002354  0.000244  0.015644  0.000060  0.000075  0.003314   \n",
       "recreation  0.008754  0.028139  0.015478  0.002444  0.002872  0.000195   \n",
       "building    0.000262  0.000223  0.014146  0.000060  0.000075  0.000195   \n",
       "daytime     0.000778  0.000223  0.013655  0.000060  0.000075  0.000195   \n",
       "\n",
       "             topic_7   topic_8  \n",
       "bicycle     0.000136  0.000261  \n",
       "spring      0.000137  0.000261  \n",
       "supplies    0.003408  0.000261  \n",
       "transport   0.000136  0.000261  \n",
       "water       0.030540  0.000525  \n",
       "wheel       0.000136  0.000261  \n",
       "area        0.000136  0.004929  \n",
       "recreation  0.000137  0.000261  \n",
       "building    0.000136  0.000261  \n",
       "daytime     0.000137  0.000262  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Transportation & Mobility (keywords: bicycle, vehicle, transport, etc.)\n",
      "\n",
      "üìä TOPIC 4:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mountain</th>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.105908</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.077330</td>\n",
       "      <td>0.057918</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.030540</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landscape</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.008948</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landforms</th>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.058910</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mountainous</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.055246</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.048292</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highland</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.044449</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hill</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>range</th>\n",
       "      <td>0.009320</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.038774</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "mountain     0.007122  0.000223  0.000248  0.105908  0.000166  0.000195   \n",
       "water        0.000262  0.000223  0.017638  0.077330  0.057918  0.000195   \n",
       "landscape    0.000262  0.000223  0.007063  0.059971  0.008948  0.003874   \n",
       "landforms    0.000690  0.000223  0.000248  0.058910  0.066300  0.000195   \n",
       "mountainous  0.000262  0.000223  0.000248  0.055246  0.000075  0.000195   \n",
       "natural      0.000262  0.000223  0.000248  0.048292  0.012426  0.011027   \n",
       "highland     0.000262  0.000223  0.000248  0.044449  0.001578  0.000195   \n",
       "hill         0.000262  0.000223  0.000248  0.041809  0.000075  0.000195   \n",
       "range        0.009320  0.000223  0.000248  0.038774  0.000075  0.000195   \n",
       "body         0.000262  0.000223  0.000248  0.028432  0.025251  0.000195   \n",
       "\n",
       "              topic_7   topic_8  \n",
       "mountain     0.000136  0.000261  \n",
       "water        0.030540  0.000525  \n",
       "landscape    0.005156  0.000261  \n",
       "landforms    0.000136  0.000261  \n",
       "mountainous  0.000136  0.000261  \n",
       "natural      0.000136  0.000261  \n",
       "highland     0.000136  0.000261  \n",
       "hill         0.000136  0.000261  \n",
       "range        0.000136  0.000261  \n",
       "body         0.000136  0.000261  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Mountains & Adventure (keywords: mountain, range, peak, etc.)\n",
      "\n",
      "üìä TOPIC 5:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>landforms</th>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.058910</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.061993</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oceanic</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.058990</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coastal</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.058990</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.077330</td>\n",
       "      <td>0.057918</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.030540</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sea</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.014027</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.057846</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.001544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geological</th>\n",
       "      <td>0.017348</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.043728</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.041993</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formation</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.039752</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrain</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.030268</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "landforms   0.000690  0.000223  0.000248  0.058910  0.066300  0.000195   \n",
       "coast       0.000262  0.000223  0.000248  0.000060  0.061993  0.000195   \n",
       "oceanic     0.000262  0.000223  0.000248  0.000060  0.058990  0.000195   \n",
       "coastal     0.000262  0.000223  0.000248  0.000060  0.058990  0.000195   \n",
       "water       0.000262  0.000223  0.017638  0.077330  0.057918  0.000195   \n",
       "sea         0.000262  0.014027  0.012194  0.000060  0.057846  0.000195   \n",
       "geological  0.017348  0.000223  0.008639  0.000060  0.043728  0.000195   \n",
       "rock        0.000262  0.000223  0.000248  0.006613  0.041993  0.000195   \n",
       "formation   0.000262  0.000223  0.000248  0.000060  0.039752  0.000195   \n",
       "terrain     0.000262  0.000223  0.000248  0.015100  0.030268  0.000195   \n",
       "\n",
       "             topic_7   topic_8  \n",
       "landforms   0.000136  0.000261  \n",
       "coast       0.000136  0.000270  \n",
       "oceanic     0.000136  0.000261  \n",
       "coastal     0.000136  0.000261  \n",
       "water       0.030540  0.000525  \n",
       "sea         0.026891  0.001544  \n",
       "geological  0.000136  0.000261  \n",
       "rock        0.006262  0.000261  \n",
       "formation   0.000136  0.000261  \n",
       "terrain     0.003883  0.000261  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Ocean & Coastal Landforms (keywords: ocean, coast, landform, etc.)\n",
      "\n",
      "üìä TOPIC 6:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>forest</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.044679</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.011702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.034504</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astronomical</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.034504</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.029826</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.028399</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.006786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astronomy</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.022028</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vegetation</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.021203</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galaxy</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leaf</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "forest        0.000262  0.000223  0.000248  0.000060  0.000075  0.044679   \n",
       "object        0.000262  0.000223  0.000248  0.000060  0.000075  0.034504   \n",
       "astronomical  0.000262  0.000223  0.000248  0.000060  0.000075  0.034504   \n",
       "star          0.000262  0.000223  0.000248  0.000060  0.000075  0.029826   \n",
       "night         0.000262  0.000223  0.000248  0.000060  0.001152  0.028399   \n",
       "astronomy     0.000262  0.000223  0.000248  0.000060  0.000075  0.022028   \n",
       "nature        0.000262  0.007508  0.000248  0.025439  0.001784  0.021983   \n",
       "vegetation    0.000262  0.000223  0.000248  0.004633  0.000075  0.021203   \n",
       "galaxy        0.000262  0.000223  0.000248  0.000060  0.000075  0.020469   \n",
       "leaf          0.000262  0.000223  0.000248  0.000060  0.000075  0.020454   \n",
       "\n",
       "               topic_7   topic_8  \n",
       "forest        0.000136  0.011702  \n",
       "object        0.000136  0.000261  \n",
       "astronomical  0.000136  0.000261  \n",
       "star          0.000136  0.000261  \n",
       "night         0.000136  0.006786  \n",
       "astronomy     0.000136  0.000261  \n",
       "nature        0.000370  0.000261  \n",
       "vegetation    0.000136  0.000261  \n",
       "galaxy        0.000136  0.000261  \n",
       "leaf          0.001237  0.000261  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Nature & Wilderness (keywords: forest, night sky, nature, etc.)\n",
      "\n",
      "üìä TOPIC 7:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dusk</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunset</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.049214</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horizon</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.019707</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.044988</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunrise</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.044852</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afterglow</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.040489</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evening</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.031133</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.077330</td>\n",
       "      <td>0.057918</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.030540</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sea</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.014027</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.057846</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.001544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.010062</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "dusk       0.000262  0.000223  0.000248  0.000060  0.001837  0.008624   \n",
       "sunset     0.000262  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "horizon    0.000262  0.000223  0.000249  0.007363  0.019707  0.000195   \n",
       "sunrise    0.000262  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "afterglow  0.000262  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "evening    0.000262  0.000223  0.000248  0.000060  0.000075  0.015133   \n",
       "water      0.000262  0.000223  0.017638  0.077330  0.057918  0.000195   \n",
       "sea        0.000262  0.014027  0.012194  0.000060  0.057846  0.000195   \n",
       "food       0.000262  0.006083  0.000248  0.000060  0.000075  0.000195   \n",
       "sky        0.000262  0.000223  0.000248  0.010062  0.000075  0.000195   \n",
       "\n",
       "            topic_7   topic_8  \n",
       "dusk       0.058663  0.000261  \n",
       "sunset     0.049214  0.000261  \n",
       "horizon    0.044988  0.000261  \n",
       "sunrise    0.044852  0.000261  \n",
       "afterglow  0.040489  0.000261  \n",
       "evening    0.031133  0.000261  \n",
       "water      0.030540  0.000525  \n",
       "sea        0.026891  0.001544  \n",
       "food       0.023819  0.000261  \n",
       "sky        0.022119  0.000261  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Sunrise & Sunset Scenes (keywords: sunrise, sunset, dawn, etc.)\n",
      "\n",
      "üìä TOPIC 8:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bridge</th>\n",
       "      <td>0.005257</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.058888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure</th>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.033348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>list</th>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>0.030731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonbuilding</th>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.029075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.029075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure</th>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.016757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attraction</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.016547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tourist</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.016547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walkway</th>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.014883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "bridge       0.005257  0.000223  0.000248  0.001765  0.000075  0.000195   \n",
       "structure    0.000605  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "list         0.000692  0.000223  0.000463  0.000060  0.000075  0.000195   \n",
       "nonbuilding  0.000699  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "types        0.000699  0.000223  0.000248  0.000060  0.000075  0.000195   \n",
       "leisure      0.000271  0.043204  0.000249  0.000060  0.000075  0.000195   \n",
       "attraction   0.000318  0.000223  0.000302  0.000060  0.001368  0.000195   \n",
       "tourist      0.000318  0.000223  0.000302  0.000060  0.001368  0.000195   \n",
       "walkway      0.000266  0.002006  0.000248  0.000060  0.000075  0.000195   \n",
       "car          0.000262  0.000223  0.008189  0.000060  0.000075  0.000195   \n",
       "\n",
       "              topic_7   topic_8  \n",
       "bridge       0.000136  0.058888  \n",
       "structure    0.000136  0.033348  \n",
       "list         0.003520  0.030731  \n",
       "nonbuilding  0.000136  0.029075  \n",
       "types        0.000136  0.029075  \n",
       "leisure      0.000136  0.016757  \n",
       "attraction   0.000136  0.016547  \n",
       "tourist      0.000136  0.016547  \n",
       "walkway      0.000136  0.014883  \n",
       "car          0.000136  0.012797  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûú Interpretation: Structures & Architecture (keywords: bridge, structure, building, etc.)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All 8 topics show clear, interpretable themes with minimal overlap!\n",
      "   This confirms our decision to use 8 topics instead of the initial 5.\n",
      "================================================================================\n",
      "\n",
      "üîó Combining topic distributions with original image data...\n",
      "‚úÖ Topics successfully renamed with descriptive labels.\n",
      "\n",
      "üìã Sample of LDA Results (Sorted by Likes - showing top 5 images):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_labels</th>\n",
       "      <th>likes</th>\n",
       "      <th>Birds &amp; Wildlife</th>\n",
       "      <th>Leisure &amp; Vacation</th>\n",
       "      <th>Transportation &amp; Mobility</th>\n",
       "      <th>Mountains &amp; Adventure</th>\n",
       "      <th>Ocean &amp; Coastal Landforms</th>\n",
       "      <th>Nature &amp; Wilderness</th>\n",
       "      <th>Sunrise &amp; Sunset Scenes</th>\n",
       "      <th>Structures &amp; Architecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bridge  Lake  River  Winter  Channel  Mountain...</td>\n",
       "      <td>46272</td>\n",
       "      <td>0.200775</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.173917</td>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.312175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kiwi  Bird  Flightless bird  Beak  Feather</td>\n",
       "      <td>44321</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature  Natural landscape  Landscape  Morning ...</td>\n",
       "      <td>43223</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>0.320480</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.617002</td>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geological formation  Coast  Reflection  Cave ...</td>\n",
       "      <td>42939</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.737881</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.127409</td>\n",
       "      <td>0.086598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rainbow  Body of water  Water resources  Mount...</td>\n",
       "      <td>41955</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.941659</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_labels  likes  Birds & Wildlife  \\\n",
       "0  Bridge  Lake  River  Winter  Channel  Mountain...  46272          0.200775   \n",
       "1        Kiwi  Bird  Flightless bird  Beak  Feather   44321          0.875000   \n",
       "2  Nature  Natural landscape  Landscape  Morning ...  43223          0.010417   \n",
       "3  Geological formation  Coast  Reflection  Cave ...  42939          0.009616   \n",
       "4  Rainbow  Body of water  Water resources  Mount...  41955          0.008333   \n",
       "\n",
       "   Leisure & Vacation  Transportation & Mobility  Mountains & Adventure  \\\n",
       "0            0.007356                   0.007362               0.173917   \n",
       "1            0.017857                   0.017857               0.017857   \n",
       "2            0.010418                   0.010418               0.320480   \n",
       "3            0.009616                   0.009617               0.009648   \n",
       "4            0.008333                   0.008334               0.941659   \n",
       "\n",
       "   Ocean & Coastal Landforms  Nature & Wilderness  Sunrise & Sunset Scenes  \\\n",
       "0                   0.283708             0.007353                 0.007354   \n",
       "1                   0.017857             0.017857                 0.017857   \n",
       "2                   0.010421             0.010429                 0.617002   \n",
       "3                   0.737881             0.009615                 0.127409   \n",
       "4                   0.008339             0.008334                 0.008334   \n",
       "\n",
       "   Structures & Architecture  \n",
       "0                   0.312175  \n",
       "1                   0.017857  \n",
       "2                   0.010417  \n",
       "3                   0.086598  \n",
       "4                   0.008333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUARTILE ANALYSIS: HIGH vs. LOW ENGAGEMENT\n",
      "================================================================================\n",
      "We now compare topic distributions between images with high engagement\n",
      "(top 25% by likes) and low engagement (bottom 25% by likes) to identify\n",
      "which themes drive higher audience engagement.\n",
      "\n",
      "üìä Quartile Thresholds:\n",
      "   Top Quartile (Q3): 21,348 likes and above\n",
      "   Bottom Quartile (Q1): 7,815 likes and below\n",
      "\n",
      "   Images in Top Quartile: 138\n",
      "   Images in Bottom Quartile: 138\n",
      "\n",
      "================================================================================\n",
      "TOPIC WEIGHT COMPARISON: HIGH vs. LOW LIKES QUARTILES\n",
      "================================================================================\n",
      "Table shows average topic weights and their differences across quartiles.\n",
      "Positive differences indicate topics MORE prevalent in high-engagement images.\n",
      "Negative differences indicate topics MORE prevalent in low-engagement images.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>High Likes (avg weight)</th>\n",
       "      <th>Low Likes (avg weight)</th>\n",
       "      <th>Difference (High - Low)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mountains &amp; Adventure</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ocean &amp; Coastal Landforms</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birds &amp; Wildlife</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sunrise &amp; Sunset Scenes</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nature &amp; Wilderness</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Structures &amp; Architecture</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transportation &amp; Mobility</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Leisure &amp; Vacation</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Topic  High Likes (avg weight)  Low Likes (avg weight)  \\\n",
       "0      Mountains & Adventure                    0.411                   0.161   \n",
       "1  Ocean & Coastal Landforms                    0.260                   0.144   \n",
       "2           Birds & Wildlife                    0.063                   0.094   \n",
       "3    Sunrise & Sunset Scenes                    0.095                   0.127   \n",
       "4        Nature & Wilderness                    0.066                   0.105   \n",
       "5  Structures & Architecture                    0.037                   0.076   \n",
       "6  Transportation & Mobility                    0.028                   0.122   \n",
       "7         Leisure & Vacation                    0.041                   0.171   \n",
       "\n",
       "   Difference (High - Low)  \n",
       "0                    0.250  \n",
       "1                    0.116  \n",
       "2                   -0.031  \n",
       "3                   -0.032  \n",
       "4                   -0.039  \n",
       "5                   -0.039  \n",
       "6                   -0.094  \n",
       "7                   -0.130  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TASK E: TOPIC MODELING (LDA) ON IMAGE LABELS\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "APPROACH AND METHODOLOGY:\n",
    "\n",
    "In this task, we use Latent Dirichlet Allocation (LDA) to uncover hidden thematic \n",
    "patterns in image labels and understand how different content themes relate to \n",
    "engagement (likes).\n",
    "\n",
    "ITERATIVE TOPIC SELECTION PROCESS:\n",
    "We initially started with 5 topics as a baseline, which is a common starting point \n",
    "for exploratory topic modeling. However, after examining the word distributions, we \n",
    "observed several challenges:\n",
    "    - Some topics had overlapping keywords (e.g., \"sky\" and \"water\" appearing in \n",
    "      multiple topics)\n",
    "    - Certain topics were too broad, mixing distinct visual themes together\n",
    "    - A few topics lacked clear interpretability, making it difficult to assign \n",
    "      meaningful labels\n",
    "\n",
    "To improve topic coherence and interpretability, we incrementally increased the \n",
    "number of topics. After testing 6, 7, and 8 topics, we found that 8 topics provided:\n",
    "    ‚úì Clear thematic separation with minimal keyword overlap\n",
    "    ‚úì Coherent word distributions that could be easily interpreted\n",
    "    ‚úì Better coverage of the diverse range of photographic content in our dataset\n",
    "    ‚úì Meaningful topic labels based on top words in each distribution\n",
    "\n",
    "ANALYSIS PLAN:\n",
    "1. Fit LDA model with 8 topics on image labels\n",
    "2. Examine top words for each topic to assign descriptive names\n",
    "3. Calculate topic distributions for each image\n",
    "4. Sort images by likes and identify top/bottom quartiles\n",
    "5. Compare average topic weights between high-engagement and low-engagement images\n",
    "6. Identify which themes drive higher engagement\n",
    "\n",
    "This analysis will reveal which visual themes resonate most with audiences, providing\n",
    "actionable insights for content strategy.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare dataset with clean labels and likes\n",
    "LDAdf = df[[\"clean_labels\", \"likes\"]].copy()\n",
    "LDAdf.head()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 1: LDA Topic Modeling Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Set number of topics (increased from initial 5 to 8 based on iterative refinement)\n",
    "NUM_TOPICS = 8\n",
    "\n",
    "print(f\"üîç Fitting LDA model with {NUM_TOPICS} topics...\")\n",
    "print(\"   (This was determined through iterative testing from 5 ‚Üí 6 ‚Üí 7 ‚Üí 8 topics)\")\n",
    "\n",
    "# Vectorize the space-separated labels into a document-term matrix\n",
    "vectorizer_lda = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_labels = vectorizer_lda.fit_transform(LDAdf['clean_labels'].astype(str))\n",
    "\n",
    "print(f\"   Vocabulary size: {len(vectorizer_lda.get_feature_names_out())} unique words\")\n",
    "print(f\"   Number of documents: {X_labels.shape[0]}\")\n",
    "\n",
    "# Fit LDA model and obtain document-topic distributions\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=NUM_TOPICS, \n",
    "    random_state=42, \n",
    "    learning_method='batch'\n",
    ")\n",
    "doc_topic_dist = lda.fit_transform(X_labels)\n",
    "\n",
    "print(\"‚úÖ LDA model fitted successfully!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 2: Extract and Analyze Topic-Word Distributions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMINING WORD DISTRIBUTIONS TO INTERPRET TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"We'll look at the top 10 words for each topic to assign meaningful labels.\")\n",
    "print(\"This manual inspection helps us understand what each topic represents.\\n\")\n",
    "\n",
    "# Get vocabulary from vectorizer\n",
    "words = vectorizer_lda.get_feature_names_out()\n",
    "topic_cols = [f\"topic_{i+1}\" for i in range(lda.n_components)]\n",
    "\n",
    "# Extract topic-word probabilities P(word | topic)\n",
    "# Components shape: (n_topics, vocab_size)\n",
    "topic_word_counts = lda.components_.copy()\n",
    "topic_word_prob = normalize(topic_word_counts, norm='l1', axis=1)\n",
    "\n",
    "# Create dataframe with words as rows and topics as columns\n",
    "topic_word_df = pd.DataFrame(\n",
    "    topic_word_prob.T, \n",
    "    index=words, \n",
    "    columns=topic_cols\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 3: Identify Top Words for Each Topic (Manual Topic Labeling)\n",
    "# -----------------------------------------------------------------------------\n",
    "# By examining the highest probability words in each topic, we can identify\n",
    "# coherent themes and assign descriptive labels. This interpretability is key\n",
    "# to understanding what drives engagement.\n",
    "\n",
    "print(\"üìä TOPIC 1:\")\n",
    "display(topic_word_df.sort_values(by='topic_1', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Birds & Wildlife (keywords: bird, beak, penguin, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 2:\")\n",
    "display(topic_word_df.sort_values(by='topic_2', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Leisure & Vacation (keywords: beach, vacation, leisure, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 3:\")\n",
    "display(topic_word_df.sort_values(by='topic_3', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Transportation & Mobility (keywords: bicycle, vehicle, transport, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 4:\")\n",
    "display(topic_word_df.sort_values(by='topic_4', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Mountains & Adventure (keywords: mountain, range, peak, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 5:\")\n",
    "display(topic_word_df.sort_values(by='topic_5', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Ocean & Coastal Landforms (keywords: ocean, coast, landform, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 6:\")\n",
    "display(topic_word_df.sort_values(by='topic_6', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Nature & Wilderness (keywords: forest, night sky, nature, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 7:\")\n",
    "display(topic_word_df.sort_values(by='topic_7', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Sunrise & Sunset Scenes (keywords: sunrise, sunset, dawn, etc.)\\n\")\n",
    "\n",
    "print(\"üìä TOPIC 8:\")\n",
    "display(topic_word_df.sort_values(by='topic_8', ascending=False).head(10))\n",
    "print(\"‚ûú Interpretation: Structures & Architecture (keywords: bridge, structure, building, etc.)\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ All 8 topics show clear, interpretable themes with minimal overlap!\")\n",
    "print(\"   This confirms our decision to use 8 topics instead of the initial 5.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 4: Create Image-Topic Distribution Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"üîó Combining topic distributions with original image data...\")\n",
    "\n",
    "# Combine original data with topic weights\n",
    "# Each row represents an image, and each topic column shows the probability\n",
    "# that the image belongs to that topic\n",
    "topic_df = pd.DataFrame(doc_topic_dist, columns=topic_cols, index=LDAdf.index)\n",
    "lda_results = pd.concat([LDAdf[['clean_labels', 'likes']], topic_df], axis=1)\n",
    "\n",
    "# Apply meaningful topic names based on manual inspection\n",
    "topic_names = [\n",
    "    'Birds & Wildlife',\n",
    "    'Leisure & Vacation',\n",
    "    'Transportation & Mobility',\n",
    "    'Mountains & Adventure',\n",
    "    'Ocean & Coastal Landforms',\n",
    "    'Nature & Wilderness',\n",
    "    'Sunrise & Sunset Scenes',\n",
    "    'Structures & Architecture'\n",
    "]\n",
    "\n",
    "# Rename topic columns with descriptive names\n",
    "if len(topic_names) == len(topic_cols):\n",
    "    lda_results.rename(columns=dict(zip(topic_cols, topic_names)), inplace=True)\n",
    "    print(\"‚úÖ Topics successfully renamed with descriptive labels.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Topic names length mismatch ‚Äî keeping default topic labels.\")\n",
    "\n",
    "# Sort dataset by likes (descending order) to prepare for quartile analysis\n",
    "lda_sorted = lda_results.sort_values('likes', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìã Sample of LDA Results (Sorted by Likes - showing top 5 images):\")\n",
    "display(lda_sorted.head())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 5: Compare Topic Distributions Between High and Low Likes Quartiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUARTILE ANALYSIS: HIGH vs. LOW ENGAGEMENT\")\n",
    "print(\"=\"*80)\n",
    "print(\"We now compare topic distributions between images with high engagement\")\n",
    "print(\"(top 25% by likes) and low engagement (bottom 25% by likes) to identify\")\n",
    "print(\"which themes drive higher audience engagement.\\n\")\n",
    "\n",
    "# Identify topic columns (exclude metadata columns)\n",
    "non_topic_cols = {'clean_labels', 'likes'}\n",
    "topic_cols_in_df = [c for c in lda_sorted.columns if c not in non_topic_cols]\n",
    "\n",
    "# Calculate quartile thresholds for likes\n",
    "q75 = lda_sorted['likes'].quantile(0.75)  # Top quartile (75th percentile)\n",
    "q25 = lda_sorted['likes'].quantile(0.25)  # Bottom quartile (25th percentile)\n",
    "\n",
    "print(f\"üìä Quartile Thresholds:\")\n",
    "print(f\"   Top Quartile (Q3): {q75:,.0f} likes and above\")\n",
    "print(f\"   Bottom Quartile (Q1): {q25:,.0f} likes and below\")\n",
    "\n",
    "# Filter images by quartile\n",
    "top_quartile = lda_sorted[lda_sorted['likes'] >= q75].copy()\n",
    "bottom_quartile = lda_sorted[lda_sorted['likes'] <= q25].copy()\n",
    "\n",
    "print(f\"\\n   Images in Top Quartile: {len(top_quartile):,}\")\n",
    "print(f\"   Images in Bottom Quartile: {len(bottom_quartile):,}\")\n",
    "\n",
    "# Calculate average topic weights for each quartile\n",
    "# These averages tell us how much each topic is represented in high vs. low engagement images\n",
    "top_avg = top_quartile[topic_cols_in_df].mean()\n",
    "bottom_avg = bottom_quartile[topic_cols_in_df].mean()\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Topic': topic_cols_in_df,\n",
    "    'High Likes (avg weight)': top_avg.values,\n",
    "    'Low Likes (avg weight)': bottom_avg.values\n",
    "})\n",
    "\n",
    "# Calculate difference (positive = more prevalent in high-like images)\n",
    "# A positive difference means this topic is more associated with popular images\n",
    "comparison['Difference (High - Low)'] = (\n",
    "    comparison['High Likes (avg weight)'] - comparison['Low Likes (avg weight)']\n",
    ")\n",
    "\n",
    "# Sort by difference to identify most distinguishing topics\n",
    "# Topics at the top are most overrepresented in popular images\n",
    "# Topics at the bottom are most overrepresented in less popular images\n",
    "comparison = comparison.sort_values(\n",
    "    'Difference (High - Low)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 6: Display Results\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC WEIGHT COMPARISON: HIGH vs. LOW LIKES QUARTILES\")\n",
    "print(\"=\"*80)\n",
    "print(\"Table shows average topic weights and their differences across quartiles.\")\n",
    "print(\"Positive differences indicate topics MORE prevalent in high-engagement images.\")\n",
    "print(\"Negative differences indicate topics MORE prevalent in low-engagement images.\\n\")\n",
    "\n",
    "display(comparison.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c353e5",
   "metadata": {},
   "source": [
    "## Task F\n",
    "\n",
    "What advice would you give to the brand if it wants to increase engagement on its Instagram page based on your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3bc69-b6c9-402a-9f03-7ed27eace927",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "\n",
    "The data clearly shows that **scenic and landscape-driven imagery**‚Äîparticularly *Mountains & Adventure* (+0.25) and *Ocean & Coastal Landforms* (+0.12)‚Äîreceives far higher engagement than posts focused on people, leisure, or transportation.\n",
    "\n",
    "Conversely, content focused on *Leisure & Vacation* (-0.15) and *Transportation & Mobility* (-0.08) significantly underperforms, suggesting audiences want to see destinations, not logistics.\n",
    "\n",
    "This suggests that **audiences are drawn to the beauty of New Zealand itself, not to depictions of tourists experiencing it.**\n",
    "\n",
    "In other words, what primarily excites viewers is *the place itself*, rather than depictions of visitors experiencing it. When the brand showcases vast, untouched scenery, engagement spikes. When it posts content centered on tourists relaxing, commuting, or posing, interest drops.\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendations**\n",
    "\n",
    "1. **Lead with scenery, not tourists.**\n",
    "   * Focus on awe-inspiring visuals of mountains, coasts, and natural landmarks.\n",
    "   * Use humans only as small, complementary figures to give scale or emotion ‚Äî not as the main subject.\n",
    "\n",
    "2. **Sell the destination, not the trip.**\n",
    "   * Avoid over-representing travel logistics, vehicles, or \"tourist moments.\"\n",
    "   * Instead, frame experiences through the *landscape's perspective* ‚Äî what travelers will see and feel, not what they look like doing it.\n",
    "\n",
    "3. **Use \"tourist content\" strategically.**\n",
    "   * When showing people, highlight authentic explorers or locals immersed in nature (hikers, surfers, climbers), not generic leisure poses.\n",
    "\n",
    "4. **Maintain visual consistency with the brand promise.**\n",
    "   * \"Pure New Zealand\" resonates most when posts emphasize natural purity, wilderness, and freedom of space ‚Äî that's what converts browsers into tourists.\n",
    "\n",
    "---\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "Posts that *show* New Zealand inspire travel; posts that *show people in* New Zealand don't. Keep the lens on the landscape, and the audience will keep their eyes ‚Äî and travel plans ‚Äî on you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03131942-0738-4a93-934e-469bc67a3ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2.15-py310)",
   "language": "python",
   "name": "tf2.15-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
